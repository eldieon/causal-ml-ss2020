{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries to use models from Grimmer et al, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO:\n",
    "\n",
    "Lasso regression is a model, similar to ridge regression, fights overfitting by allowing a higher mean square error, that will be beneficial on the long run when running on test data. This is obtained by adding a regularization penalty, which is represented by alpha in the library. [Library documentation available here.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#examples-using-sklearn-linear-model-lasso)\n",
    "\n",
    "```class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net:\n",
    "\n",
    "Elastic net regression combines the regularization penalty of both Lasso and Ridge regression models. [Library documentation available here.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#)\n",
    "\n",
    "```class sklearn.linear_model.ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')```\n",
    "\n",
    "An example elaborating both Lasso and Elastic Net regression models can be found [here.](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find It: Finding Heterogeneous Treatment Effects\n",
    "\n",
    "The method adapts the Support Vector Machine classifier by placing separate LASSO constraints over the pre-treatment parameters and causal heterogeneity parameters of interest. This package can be only found in R. [Package can be found here.](https://cran.r-project.org/web/packages/FindIt/FindIt.pdf)\n",
    "\n",
    "```FindIt( model.treat, model.main, model.int, data = NULL, type = \"binary\", treat.type = \"multiple\", nway, search.lambdas = TRUE, lambdas = NULL, make.twoway = TRUE, make.allway = TRUE, wts = 1, scale.c = 1, scale.int = 1, fit.glmnet = TRUE, make.reference = TRUE, reference.main = NULL, threshold = 0.999999 )```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian GLM:\n",
    "\n",
    "A modified glm function, that finds an approximate posterior mode and variance using extensions of the classical generalized linear model computations. This is only officially available in R [here](https://www.rdocumentation.org/packages/arm/versions/1.11-1/topics/bayesglm), but also available an open source github project (unlicensed) to use in python [here.](https://github.com/dchudz/BayesGLM)\n",
    "\n",
    "```bayesglm (formula, family = gaussian, data, weights, subset, na.action, start = NULL, etastart, mustart, offset, control = list(...), model = TRUE, method = \"glm.fit\", x = FALSE, y = TRUE, contrasts = NULL, drop.unused.levels = TRUE, prior.mean = 0, prior.scale = NULL, prior.df = 1, prior.mean.for.intercept = 0, prior.scale.for.intercept = NULL, prior.df.for.intercept = 1, min.prior.scale=1e-12, scaled = TRUE, keep.order=TRUE, drop.baseline=TRUE, maxit=100, print.unnormalized.log.posterior=FALSE, Warning=TRUE,...)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART: Bayesian Additive Regression Trees:\n",
    "\n",
    "BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood.\n",
    "\n",
    "Package is available in R [here.](https://cran.r-project.org/web/packages/BART/BART.pdf)\n",
    "\n",
    "Licensed Github project to use available [here.](https://github.com/JakeColtman/bartpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor:\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. [Library documentation available here.](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "```class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRLS: Kernel Regularized Least Squares:\n",
    "\n",
    "\n",
    "Function implements Kernel-Based Regularized Least Squares (KRLS), a machine learning method described in Hainmueller and Hazlett (2014) that allows users to solve regression and classification problems without manual specification search and strong functional form assumptions.\n",
    "\n",
    "This package is available in R. Yet to be found or made in python. [Available here.](https://www.rdocumentation.org/packages/KRLS/versions/1.0-0/topics/krls)\n",
    "\n",
    "```krls(X = NULL, y = NULL, whichkernel = \"gaussian\", lambda = NULL, sigma = NULL, derivative = TRUE, binary= TRUE, vcov=TRUE, print.level = 1,L=NULL,U=NULL,tol=NULL,eigtrunc=NULL)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM):\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n",
    "\n",
    "[Library documentation available here.](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
